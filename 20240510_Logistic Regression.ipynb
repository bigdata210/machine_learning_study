{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1c2a97",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85523c68",
   "metadata": {},
   "source": [
    "## 분류(Classification)\n",
    "- 미지의 입력 데이터에서 결과가 어떤 종류의 값으로 분류될 수 있는지 예측하는 것\n",
    "\n",
    "### Logistic Regression 알고리즘\n",
    "- Training Data 특성과 분포를 나타내는 최적의 직선(Linear Regression)을 찾음\n",
    "- 직선을 기준으로 데이터를 위(1) 또는 아래(0)으로 분류 해주는 알고리즘\n",
    "\n",
    "### Classification - sigmoid function\n",
    "- 출력값 y가 1 또는 0 만을 가져야만 하는 분류 시스템에서 함수값으로 0~1 사이의 값을 가지는 sigmoid 함수를 사용할 수 있음\n",
    "- sigmoid 계산 값이 0.5보다 큼 => 결과가 1이 나올 확률이 높으므로 y값을 1로 정의\n",
    "- sigmoid 계산 값이 0.5보다 미만 => 결과가 0이 나올 확률이 높으므로 y값을 0으로 정의 \n",
    "\n",
    "### 손실함수(loss function)\n",
    "- 분류 시스템 최종 출력 값 y는 sigmoid 함수에 의해 논리적으로 1 또는 0 값을 가지기 때문에 연속 값을 갖는 선형회귀 때와는 다른 손실 함수가 필요함\n",
    "- 손실함수 Cross entropy 활용\n",
    "- 우도 함수 : 입력 x에 대해 정답 t가 발생될 확률을 나타낸 함수\n",
    "- 확률은 독립적이므로 각 입력 데이터의 발생확률을 곱해서 우도 함수를 나타냄\n",
    "- 우도 함수가 최대 => 발생 확률이 가장 높다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9da98",
   "metadata": {},
   "source": [
    "### training data 준비 \n",
    "- 공부시간(x), Fail/Pass(z) : 공부시간이 14시간 이상이면 Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39077e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.06794623]] ,W.shape = (1, 1) , b = [0.24140093] , b.shape = (1,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10,1)\n",
    "t_data = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1]).reshape(10,1)\n",
    "\n",
    "# 임의의 직선 z=Wx + b 정의 (임의의 값으로 가중치 W, 바이어스 b 초기화)\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W =\", W, \",W.shape =\", W.shape,\", b =\", b, \", b.shape =\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6036b2",
   "metadata": {},
   "source": [
    "### 손실함수 정의 E(W, b) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64087f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x) )\n",
    "\n",
    "def loss_func(x, t):\n",
    "    delta = 1e-7\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # crosss-entropy\n",
    "    return -np.sum( t*np.log(y+ delta) +   # log의 무한대 방지를 위해 임의의 delta를 더함\n",
    "                     (1-t)*np.log(1-y+delta) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3764b",
   "metadata": {},
   "source": [
    "### 수치미분 numerical_derivative 및 utility 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc8043e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659ad4a2",
   "metadata": {},
   "source": [
    "### 손실함수 값 계산 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc1d5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy\n",
    "    return -np.sum( t*np.log(y + delta) + (1-t)*np.log((1- y)+ delta))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c251ec",
   "metadata": {},
   "source": [
    "### 학습 후 임의의 데이터에 대해 미래 값 예측 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "762c0bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력변수 x, t : numpy type\n",
    "def predict(x):\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y > 0.5:\n",
    "        result = 1  # True\n",
    "    else: \n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82b07833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 error value = 0.5691948103046798 W = [[1.23072213]] b = [-15.88066464]\n",
      "step = 400 error value = 0.56017723552856 W = [[1.24524173]] b = [-16.07040421]\n",
      "step = 800 error value = 0.5515249661651015 W = [[1.25946592]] b = [-16.25625639]\n",
      "step = 1200 error value = 0.5432123478988171 W = [[1.27340971]] b = [-16.43841996]\n",
      "step = 1600 error value = 0.5352162706888679 W = [[1.28708694]] b = [-16.61707753]\n",
      "step = 2000 error value = 0.5275158483370084 W = [[1.3005103]] b = [-16.79239728]\n",
      "step = 2400 error value = 0.5200921463874416 W = [[1.31369156]] b = [-16.9645345]\n",
      "step = 2800 error value = 0.5129279499390839 W = [[1.32664158]] b = [-17.1336329]\n",
      "step = 3200 error value = 0.5060075646033602 W = [[1.33937047]] b = [-17.29982577]\n",
      "step = 3600 error value = 0.49931664513417545 W = [[1.35188759]] b = [-17.46323701]\n",
      "step = 4000 error value = 0.49284204727631437 W = [[1.3642017]] b = [-17.62398198]\n",
      "step = 4400 error value = 0.48657169918748927 W = [[1.37632095]] b = [-17.78216834]\n",
      "step = 4800 error value = 0.48049448943619455 W = [[1.38825296]] b = [-17.93789672]\n",
      "step = 5200 error value = 0.4746001690962338 W = [[1.40000487]] b = [-18.09126133]\n",
      "step = 5600 error value = 0.46887926587805306 W = [[1.4115834]] b = [-18.24235053]\n",
      "step = 6000 error value = 0.46332300857814684 W = [[1.42299482]] b = [-18.39124731]\n",
      "step = 6400 error value = 0.45792326040513737 W = [[1.43424506]] b = [-18.53802974]\n",
      "step = 6800 error value = 0.4526724599698478 W = [[1.4453397]] b = [-18.68277136]\n",
      "step = 7200 error value = 0.447563568913937 W = [[1.45628401]] b = [-18.82554155]\n",
      "step = 7600 error value = 0.4425900253078055 W = [[1.46708296]] b = [-18.96640581]\n",
      "step = 8000 error value = 0.4377457020771945 W = [[1.47774125]] b = [-19.10542611]\n",
      "step = 8400 error value = 0.4330248698260941 W = [[1.48826334]] b = [-19.24266112]\n",
      "step = 8800 error value = 0.4284221635138377 W = [[1.49865345]] b = [-19.37816644]\n",
      "step = 9200 error value = 0.42393255252013734 W = [[1.50891559]] b = [-19.51199484]\n",
      "step = 9600 error value = 0.41955131369607734 W = [[1.51905357]] b = [-19.64419645]\n",
      "step = 10000 error value = 0.415274007053064 W = [[1.529071]] b = [-19.77481892]\n"
     ]
    }
   ],
   "source": [
    "# 학습률\n",
    "learning_rate = 1e-2\n",
    "\n",
    "f = lambda x : loss_func(x_data, t_data)\n",
    "    \n",
    "for step in range(10001):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step =\", step, \"error value =\", error_val(x_data, t_data), \"W =\", W, \"b =\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9cd52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.53575008e-07]] 0\n"
     ]
    }
   ],
   "source": [
    "# 공부시간이 3시간일 경우 fail\n",
    "(real_val, logical_val) = predict(3)\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab0e92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99801349]] 1\n"
     ]
    }
   ],
   "source": [
    "# 공부시간 17시간일 경우 Pass\n",
    "## 합격할 확률 99.80%로 합격\n",
    "(real_val, logical_val) = predict(17)\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549afa63",
   "metadata": {},
   "source": [
    "## multi-variable logistic regression\n",
    "### training data 준비 \n",
    "- 예습시간, 복습시간(x) Fail/Pass(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebac72a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W = [[0.07382132]\n",
      " [0.01847308]] ,W.shape = (2, 1) , b = [0.43597783] , b.shape = (1,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([[2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 4], [18, 7]])\n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(9,1)\n",
    "\n",
    "# 임의의 직선 z= W1x1 + W2x2 + b 정의 (임의의 값으로 가중치 W, 바이어스 b 초기화)\n",
    "W = np.random.rand(2,1)\n",
    "b = np.random.rand(1)\n",
    "print(\"W =\", W, \",W.shape =\", W.shape,\", b =\", b, \", b.shape =\", b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d9c05",
   "metadata": {},
   "source": [
    "### 손실함수 E(W, b) 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f976729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x) )\n",
    "\n",
    "def loss_func(x, t):\n",
    "    delta = 1e-7\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # crosss-entropy\n",
    "    return -np.sum( t*np.log(y+ delta) +   # log의 무한대 방지를 위해 임의의 delta를 더함\n",
    "                     (1-t)*np.log(1-y+delta) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37192324",
   "metadata": {},
   "source": [
    "### 수치미분 numerical_derivative 및 utility 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7252da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b3d578",
   "metadata": {},
   "source": [
    "### 손실함수 값 계산 및 예측 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ea5a544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy\n",
    "    return -np.sum( t*np.log(y + delta) + (1-t)*np.log((1- y)+ delta))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "708f8a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력변수 x, t : numpy type\n",
    "def predict(x):\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y > 0.5:\n",
    "        result = 1  # True\n",
    "    else: \n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e82c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 0 error value = 0.40425125933041506 W = [[1.27970163]\n",
      " [0.31388876]] b = [-13.32552684]\n",
      "step = 400 error value = 0.39421410428233644 W = [[1.29444428]\n",
      " [0.32505692]] b = [-13.52534405]\n",
      "step = 800 error value = 0.3846791370771161 W = [[1.30876068]\n",
      " [0.33602733]] b = [-13.72009215]\n",
      "step = 1200 error value = 0.375606260467164 W = [[1.32268384]\n",
      " [0.34679683]] b = [-13.91005683]\n",
      "step = 1600 error value = 0.3669600251775819 W = [[1.33624263]\n",
      " [0.35736395]] b = [-14.09549589]\n",
      "step = 2000 error value = 0.35870891300480956 W = [[1.34946244]\n",
      " [0.36772874]] b = [-14.27664321]\n",
      "step = 2400 error value = 0.3508247548932601 W = [[1.36236562]\n",
      " [0.37789242]] b = [-14.45371194]\n",
      "step = 2800 error value = 0.3432822549417611 W = [[1.37497194]\n",
      " [0.38785725]] b = [-14.62689722]\n",
      "step = 3200 error value = 0.33605859815348654 W = [[1.38729897]\n",
      " [0.39762624]] b = [-14.79637838]\n",
      "step = 3600 error value = 0.32913312486326424 W = [[1.39936239]\n",
      " [0.40720301]] b = [-14.9623209]\n",
      "step = 4000 error value = 0.32248705862503724 W = [[1.41117625]\n",
      " [0.41659164]] b = [-15.12487793]\n",
      "step = 4400 error value = 0.31610327725563875 W = [[1.42275319]\n",
      " [0.42579655]] b = [-15.2841917]\n",
      "step = 4800 error value = 0.30996611895197 W = [[1.43410467]\n",
      " [0.43482236]] b = [-15.44039464]\n",
      "step = 5200 error value = 0.3040612171000838 W = [[1.44524109]\n",
      " [0.44367386]] b = [-15.59361042]\n",
      "step = 5600 error value = 0.29837535870734266 W = [[1.45617195]\n",
      " [0.45235589]] b = [-15.74395475]\n",
      "step = 6000 error value = 0.29289636240673417 W = [[1.46690596]\n",
      " [0.46087332]] b = [-15.89153618]\n",
      "step = 6400 error value = 0.2876129727756046 W = [[1.47745115]\n",
      " [0.46923096]] b = [-16.03645675]\n",
      "step = 6800 error value = 0.282514768334349 W = [[1.48781491]\n",
      " [0.47743362]] b = [-16.1788125]\n",
      "step = 7200 error value = 0.27759208108095634 W = [[1.49800413]\n",
      " [0.48548596]] b = [-16.31869407]\n",
      "step = 7600 error value = 0.2728359258066653 W = [[1.5080252 ]\n",
      " [0.49339259]] b = [-16.45618705]\n",
      "step = 8000 error value = 0.2682379377489383 W = [[1.5178841 ]\n",
      " [0.50115796]] b = [-16.59137247]\n",
      "step = 8400 error value = 0.2637903173859365 W = [[1.52758641]\n",
      " [0.50878642]] b = [-16.72432707]\n",
      "step = 8800 error value = 0.2594857813782643 W = [[1.5371374 ]\n",
      " [0.51628217]] b = [-16.85512367]\n",
      "step = 9200 error value = 0.25531751882555215 W = [[1.546542  ]\n",
      " [0.52364929]] b = [-16.98383144]\n",
      "step = 9600 error value = 0.25127915213851787 W = [[1.55580487]\n",
      " [0.5308917 ]] b = [-17.11051615]\n",
      "step = 10000 error value = 0.2473647019351083 W = [[1.56493043]\n",
      " [0.5380132 ]] b = [-17.23524043]\n",
      "step = 10400 error value = 0.24356855545915054 W = [[1.57392284]\n",
      " [0.54501744]] b = [-17.35806395]\n",
      "step = 10800 error value = 0.23988543809328025 W = [[1.58278607]\n",
      " [0.55190794]] b = [-17.47904362]\n",
      "step = 11200 error value = 0.23631038759986753 W = [[1.59152389]\n",
      " [0.55868809]] b = [-17.59823379]\n",
      "step = 11600 error value = 0.23283873077433442 W = [[1.60013988]\n",
      " [0.56536115]] b = [-17.71568637]\n",
      "step = 12000 error value = 0.22946606223889743 W = [[1.60863746]\n",
      " [0.57193026]] b = [-17.83145103]\n",
      "step = 12400 error value = 0.22618822514066297 W = [[1.61701989]\n",
      " [0.57839843]] b = [-17.94557528]\n",
      "step = 12800 error value = 0.22300129354882933 W = [[1.6252903 ]\n",
      " [0.58476855]] b = [-18.05810463]\n",
      "step = 13200 error value = 0.2199015563715919 W = [[1.63345166]\n",
      " [0.59104343]] b = [-18.16908271]\n",
      "step = 13600 error value = 0.21688550263614936 W = [[1.64150683]\n",
      " [0.59722574]] b = [-18.27855136]\n",
      "step = 14000 error value = 0.21394980799372204 W = [[1.64945856]\n",
      " [0.60331806]] b = [-18.38655073]\n",
      "step = 14400 error value = 0.21109132232802094 W = [[1.65730946]\n",
      " [0.60932287]] b = [-18.49311936]\n",
      "step = 14800 error value = 0.2083070583600564 W = [[1.66506207]\n",
      " [0.61524255]] b = [-18.59829431]\n",
      "step = 15200 error value = 0.20559418115396952 W = [[1.67271882]\n",
      " [0.62107941]] b = [-18.70211121]\n",
      "step = 15600 error value = 0.20294999843947645 W = [[1.68028204]\n",
      " [0.62683565]] b = [-18.8046043]\n",
      "step = 16000 error value = 0.20037195167572822 W = [[1.68775397]\n",
      " [0.63251339]] b = [-18.90580658]\n",
      "step = 16400 error value = 0.19785760778937794 W = [[1.69513678]\n",
      " [0.63811469]] b = [-19.00574979]\n",
      "step = 16800 error value = 0.1954046515268195 W = [[1.70243255]\n",
      " [0.64364152]] b = [-19.10446454]\n",
      "step = 17200 error value = 0.1930108783668687 W = [[1.7096433 ]\n",
      " [0.64909578]] b = [-19.20198032]\n",
      "step = 17600 error value = 0.1906741879453436 W = [[1.71677098]\n",
      " [0.6544793 ]] b = [-19.29832557]\n",
      "step = 18000 error value = 0.1883925779482933 W = [[1.72381745]\n",
      " [0.65979385]] b = [-19.39352774]\n",
      "step = 18400 error value = 0.1861641384343917 W = [[1.73078452]\n",
      " [0.66504112]] b = [-19.48761331]\n",
      "step = 18800 error value = 0.18398704655134412 W = [[1.73767394]\n",
      " [0.67022276]] b = [-19.58060787]\n",
      "step = 19200 error value = 0.1818595616140554 W = [[1.74448741]\n",
      " [0.67534036]] b = [-19.67253613]\n",
      "step = 19600 error value = 0.17978002051564446 W = [[1.75122657]\n",
      " [0.68039545]] b = [-19.76342197]\n",
      "step = 20000 error value = 0.17774683344498218 W = [[1.75789299]\n",
      " [0.6853895 ]] b = [-19.85328847]\n",
      "step = 20400 error value = 0.1757584798867369 W = [[1.7644882 ]\n",
      " [0.69032393]] b = [-19.94215799]\n",
      "step = 20800 error value = 0.17381350488220493 W = [[1.77101371]\n",
      " [0.69520013]] b = [-20.03005212]\n",
      "step = 21200 error value = 0.171910515531065 W = [[1.77747093]\n",
      " [0.70001944]] b = [-20.11699179]\n",
      "step = 21600 error value = 0.17004817771597702 W = [[1.78386128]\n",
      " [0.70478313]] b = [-20.20299725]\n",
      "step = 22000 error value = 0.16822521303334495 W = [[1.79018609]\n",
      " [0.70949245]] b = [-20.28808811]\n",
      "step = 22400 error value = 0.1664403959153495 W = [[1.79644669]\n",
      " [0.71414862]] b = [-20.3722834]\n",
      "step = 22800 error value = 0.16469255092909035 W = [[1.80264434]\n",
      " [0.71875279]] b = [-20.45560153]\n",
      "step = 23200 error value = 0.1629805502404093 W = [[1.80878027]\n",
      " [0.72330609]] b = [-20.53806037]\n",
      "step = 23600 error value = 0.1613033112305306 W = [[1.8148557 ]\n",
      " [0.72780962]] b = [-20.61967725]\n",
      "step = 24000 error value = 0.15965979425487317 W = [[1.82087178]\n",
      " [0.73226444]] b = [-20.70046898]\n",
      "step = 24400 error value = 0.15804900053418466 W = [[1.82682964]\n",
      " [0.73667157]] b = [-20.78045187]\n",
      "step = 24800 error value = 0.15646997016891268 W = [[1.83273039]\n",
      " [0.74103201]] b = [-20.85964176]\n",
      "step = 25200 error value = 0.15492178026843673 W = [[1.83857509]\n",
      " [0.74534672]] b = [-20.93805404]\n",
      "step = 25600 error value = 0.1534035431874358 W = [[1.84436478]\n",
      " [0.74961664]] b = [-21.01570364]\n",
      "step = 26000 error value = 0.15191440486225322 W = [[1.85010047]\n",
      " [0.75384267]] b = [-21.09260508]\n",
      "step = 26400 error value = 0.1504535432407979 W = [[1.85578315]\n",
      " [0.75802569]] b = [-21.16877247]\n",
      "step = 26800 error value = 0.1490201667996567 W = [[1.86141378]\n",
      " [0.76216656]] b = [-21.24421952]\n",
      "step = 27200 error value = 0.14761351314305912 W = [[1.86699327]\n",
      " [0.76626611]] b = [-21.31895956]\n",
      "step = 27600 error value = 0.14623284767820693 W = [[1.87252255]\n",
      " [0.77032515]] b = [-21.39300557]\n",
      "step = 28000 error value = 0.14487746236233076 W = [[1.87800248]\n",
      " [0.77434444]] b = [-21.46637017]\n",
      "step = 28400 error value = 0.14354667451685676 W = [[1.88343394]\n",
      " [0.77832477]] b = [-21.53906564]\n",
      "step = 28800 error value = 0.14223982570458882 W = [[1.88881777]\n",
      " [0.78226685]] b = [-21.61110392]\n",
      "step = 29200 error value = 0.14095628066596194 W = [[1.89415477]\n",
      " [0.78617143]] b = [-21.68249667]\n",
      "step = 29600 error value = 0.13969542631083942 W = [[1.89944574]\n",
      " [0.79003918]] b = [-21.7532552]\n",
      "step = 30000 error value = 0.13845667076241486 W = [[1.90469146]\n",
      " [0.79387079]] b = [-21.82339057]\n",
      "step = 30400 error value = 0.1372394424501315 W = [[1.90989269]\n",
      " [0.79766692]] b = [-21.89291353]\n",
      "step = 30800 error value = 0.13604318924874148 W = [[1.91505016]\n",
      " [0.80142822]] b = [-21.96183455]\n",
      "step = 31200 error value = 0.134867377660748 W = [[1.92016459]\n",
      " [0.8051553 ]] b = [-22.03016386]\n",
      "step = 31600 error value = 0.13371149203969787 W = [[1.92523669]\n",
      " [0.80884879]] b = [-22.09791141]\n",
      "step = 32000 error value = 0.1325750338519878 W = [[1.93026714]\n",
      " [0.81250927]] b = [-22.16508691]\n",
      "step = 32400 error value = 0.1314575209749269 W = [[1.93525661]\n",
      " [0.81613731]] b = [-22.23169985]\n",
      "step = 32800 error value = 0.13035848702905364 W = [[1.94020575]\n",
      " [0.8197335 ]] b = [-22.29775945]\n",
      "step = 33200 error value = 0.12927748074274772 W = [[1.94511521]\n",
      " [0.82329836]] b = [-22.36327473]\n",
      "step = 33600 error value = 0.1282140653472855 W = [[1.94998559]\n",
      " [0.82683244]] b = [-22.4282545]\n",
      "step = 34000 error value = 0.1271678180007397 W = [[1.95481752]\n",
      " [0.83033626]] b = [-22.49270735]\n",
      "step = 34400 error value = 0.12613832923898932 W = [[1.95961159]\n",
      " [0.83381032]] b = [-22.55664164]\n",
      "step = 34800 error value = 0.12512520245258849 W = [[1.96436838]\n",
      " [0.83725512]] b = [-22.62006559]\n",
      "step = 35200 error value = 0.12412805338780188 W = [[1.96908845]\n",
      " [0.84067115]] b = [-22.68298717]\n",
      "step = 35600 error value = 0.12314650967079734 W = [[1.97377236]\n",
      " [0.84405888]] b = [-22.74541421]\n",
      "step = 36000 error value = 0.12218021035351634 W = [[1.97842066]\n",
      " [0.84741876]] b = [-22.80735433]\n",
      "step = 36400 error value = 0.12122880548022796 W = [[1.98303387]\n",
      " [0.85075125]] b = [-22.86881499]\n",
      "step = 36800 error value = 0.12029195567362626 W = [[1.98761251]\n",
      " [0.85405678]] b = [-22.92980348]\n",
      "step = 37200 error value = 0.11936933173942077 W = [[1.99215709]\n",
      " [0.85733578]] b = [-22.99032693]\n",
      "step = 37600 error value = 0.11846061428854433 W = [[1.99666811]\n",
      " [0.86058867]] b = [-23.05039229]\n",
      "step = 38000 error value = 0.11756549337598927 W = [[2.00114606]\n",
      " [0.86381586]] b = [-23.11000639]\n",
      "step = 38400 error value = 0.11668366815544676 W = [[2.0055914 ]\n",
      " [0.86701775]] b = [-23.16917588]\n",
      "step = 38800 error value = 0.11581484654900982 W = [[2.01000461]\n",
      " [0.87019472]] b = [-23.22790728]\n",
      "step = 39200 error value = 0.11495874493108522 W = [[2.01438614]\n",
      " [0.87334716]] b = [-23.28620696]\n",
      "step = 39600 error value = 0.11411508782586464 W = [[2.01873643]\n",
      " [0.87647544]] b = [-23.34408116]\n",
      "step = 40000 error value = 0.11328360761766713 W = [[2.02305593]\n",
      " [0.87957993]] b = [-23.40153599]\n",
      "step = 40400 error value = 0.11246404427352018 W = [[2.02734506]\n",
      " [0.88266097]] b = [-23.45857742]\n",
      "step = 40800 error value = 0.11165614507740604 W = [[2.03160424]\n",
      " [0.88571892]] b = [-23.51521129]\n",
      "step = 41200 error value = 0.11085966437553678 W = [[2.03583388]\n",
      " [0.88875412]] b = [-23.57144334]\n",
      "step = 41600 error value = 0.11007436333221729 W = [[2.04003438]\n",
      " [0.8917669 ]] b = [-23.62727916]\n",
      "step = 42000 error value = 0.10930000969570887 W = [[2.04420613]\n",
      " [0.89475758]] b = [-23.68272426]\n",
      "step = 42400 error value = 0.10853637757366265 W = [[2.04834954]\n",
      " [0.89772649]] b = [-23.73778401]\n",
      "step = 42800 error value = 0.10778324721768938 W = [[2.05246496]\n",
      " [0.90067393]] b = [-23.79246368]\n",
      "step = 43200 error value = 0.10704040481657126 W = [[2.05655277]\n",
      " [0.90360021]] b = [-23.84676842]\n",
      "step = 43600 error value = 0.10630764229774851 W = [[2.06061334]\n",
      " [0.90650564]] b = [-23.90070331]\n",
      "step = 44000 error value = 0.10558475713674481 W = [[2.06464703]\n",
      " [0.9093905 ]] b = [-23.95427329]\n",
      "step = 44400 error value = 0.10487155217406298 W = [[2.06865417]\n",
      " [0.91225507]] b = [-24.00748323]\n",
      "step = 44800 error value = 0.10416783543930029 W = [[2.07263513]\n",
      " [0.91509965]] b = [-24.06033789]\n",
      "step = 45200 error value = 0.10347341998210419 W = [[2.07659022]\n",
      " [0.9179245 ]] b = [-24.11284193]\n",
      "step = 45600 error value = 0.10278812370971964 W = [[2.08051979]\n",
      " [0.92072989]] b = [-24.16499994]\n",
      "step = 46000 error value = 0.10211176923071778 W = [[2.08442415]\n",
      " [0.92351609]] b = [-24.21681641]\n",
      "step = 46400 error value = 0.10144418370476155 W = [[2.08830363]\n",
      " [0.92628336]] b = [-24.26829574]\n",
      "step = 46800 error value = 0.10078519869807025 W = [[2.09215854]\n",
      " [0.92903195]] b = [-24.31944224]\n",
      "step = 47200 error value = 0.10013465004433261 W = [[2.09598917]\n",
      " [0.9317621 ]] b = [-24.37026017]\n",
      "step = 47600 error value = 0.0994923777108413 W = [[2.09979584]\n",
      " [0.93447407]] b = [-24.42075367]\n",
      "step = 48000 error value = 0.09885822566963814 W = [[2.10357884]\n",
      " [0.93716808]] b = [-24.47092683]\n",
      "step = 48400 error value = 0.09823204177342594 W = [[2.10733846]\n",
      " [0.93984438]] b = [-24.52078365]\n",
      "step = 48800 error value = 0.09761367763606094 W = [[2.11107498]\n",
      " [0.94250319]] b = [-24.57032805]\n",
      "step = 49200 error value = 0.09700298851741117 W = [[2.11478867]\n",
      " [0.94514473]] b = [-24.61956391]\n",
      "step = 49600 error value = 0.09639983321243843 W = [[2.11847983]\n",
      " [0.94776924]] b = [-24.66849499]\n",
      "step = 50000 error value = 0.09580407394424151 W = [[2.12214871]\n",
      " [0.95037692]] b = [-24.71712503]\n",
      "step = 50400 error value = 0.09521557626102044 W = [[2.12579557]\n",
      " [0.95296799]] b = [-24.76545767]\n",
      "step = 50800 error value = 0.09463420893666347 W = [[2.12942069]\n",
      " [0.95554266]] b = [-24.81349649]\n",
      "step = 51200 error value = 0.0940598438749065 W = [[2.13302431]\n",
      " [0.95810113]] b = [-24.86124503]\n",
      "step = 51600 error value = 0.0934923560168711 W = [[2.13660669]\n",
      " [0.96064359]] b = [-24.90870672]\n",
      "step = 52000 error value = 0.09293162325184125 W = [[2.14016807]\n",
      " [0.96317026]] b = [-24.95588498]\n",
      "step = 52400 error value = 0.0923775263311984 W = [[2.14370869]\n",
      " [0.96568132]] b = [-25.00278315]\n",
      "step = 52800 error value = 0.09182994878527684 W = [[2.1472288 ]\n",
      " [0.96817697]] b = [-25.04940449]\n",
      "step = 53200 error value = 0.091288776843177 W = [[2.15072863]\n",
      " [0.97065738]] b = [-25.09575223]\n",
      "step = 53600 error value = 0.09075389935527221 W = [[2.1542084 ]\n",
      " [0.97312274]] b = [-25.14182954]\n",
      "step = 54000 error value = 0.09022520771838667 W = [[2.15766834]\n",
      " [0.97557324]] b = [-25.18763953]\n",
      "step = 54400 error value = 0.08970259580350798 W = [[2.16110869]\n",
      " [0.97800905]] b = [-25.23318527]\n",
      "step = 54800 error value = 0.08918595988594102 W = [[2.16452964]\n",
      " [0.98043034]] b = [-25.27846975]\n",
      "step = 55200 error value = 0.08867519857783968 W = [[2.16793143]\n",
      " [0.98283728]] b = [-25.32349593]\n",
      "step = 55600 error value = 0.08817021276295509 W = [[2.17131426]\n",
      " [0.98523004]] b = [-25.36826673]\n",
      "step = 56000 error value = 0.08767090553354413 W = [[2.17467834]\n",
      " [0.98760879]] b = [-25.412785]\n",
      "step = 56400 error value = 0.08717718212941382 W = [[2.17802388]\n",
      " [0.98997369]] b = [-25.45705354]\n",
      "step = 56800 error value = 0.08668894987892567 W = [[2.18135107]\n",
      " [0.9923249 ]] b = [-25.50107513]\n",
      "step = 57200 error value = 0.08620611814195911 W = [[2.18466012]\n",
      " [0.99466256]] b = [-25.54485249]\n",
      "step = 57600 error value = 0.0857285982546828 W = [[2.18795122]\n",
      " [0.99698685]] b = [-25.58838828]\n",
      "step = 58000 error value = 0.08525630347618292 W = [[2.19122456]\n",
      " [0.9992979 ]] b = [-25.63168514]\n",
      "step = 58400 error value = 0.0847891489367526 W = [[2.19448033]\n",
      " [1.00159587]] b = [-25.67474566]\n",
      "step = 58800 error value = 0.08432705158784236 W = [[2.19771872]\n",
      " [1.00388091]] b = [-25.71757239]\n",
      "step = 59200 error value = 0.08386993015363164 W = [[2.20093991]\n",
      " [1.00615315]] b = [-25.76016783]\n",
      "step = 59600 error value = 0.08341770508413127 W = [[2.20414408]\n",
      " [1.00841274]] b = [-25.80253444]\n",
      "step = 60000 error value = 0.08297029850971889 W = [[2.20733141]\n",
      " [1.01065981]] b = [-25.84467466]\n",
      "step = 60400 error value = 0.08252763419715992 W = [[2.21050207]\n",
      " [1.01289451]] b = [-25.88659088]\n",
      "step = 60800 error value = 0.08208963750696359 W = [[2.21365624]\n",
      " [1.01511697]] b = [-25.92828544]\n",
      "step = 61200 error value = 0.08165623535207979 W = [[2.21679408]\n",
      " [1.01732732]] b = [-25.96976066]\n",
      "step = 61600 error value = 0.0812273561578385 W = [[2.21991576]\n",
      " [1.01952569]] b = [-26.01101882]\n",
      "step = 62000 error value = 0.08080292982317945 W = [[2.22302145]\n",
      " [1.02171221]] b = [-26.05206217]\n",
      "step = 62400 error value = 0.08038288768300075 W = [[2.2261113 ]\n",
      " [1.02388701]] b = [-26.09289291]\n",
      "step = 62800 error value = 0.07996716247169283 W = [[2.22918548]\n",
      " [1.02605021]] b = [-26.13351321]\n",
      "step = 63200 error value = 0.0795556882877788 W = [[2.23224414]\n",
      " [1.02820192]] b = [-26.17392524]\n",
      "step = 63600 error value = 0.0791484005595872 W = [[2.23528744]\n",
      " [1.03034228]] b = [-26.21413108]\n",
      "step = 64000 error value = 0.07874523601199132 W = [[2.23831554]\n",
      " [1.03247141]] b = [-26.25413282]\n",
      "step = 64400 error value = 0.07834613263410144 W = [[2.24132857]\n",
      " [1.0345894 ]] b = [-26.29393251]\n",
      "step = 64800 error value = 0.07795102964795438 W = [[2.24432669]\n",
      " [1.03669639]] b = [-26.33353217]\n",
      "step = 65200 error value = 0.07755986747809418 W = [[2.24731004]\n",
      " [1.03879249]] b = [-26.37293378]\n",
      "step = 65600 error value = 0.07717258772206925 W = [[2.25027877]\n",
      " [1.0408778 ]] b = [-26.4121393]\n",
      "step = 66000 error value = 0.07678913312179063 W = [[2.25323302]\n",
      " [1.04295244]] b = [-26.45115066]\n",
      "step = 66400 error value = 0.07640944753569105 W = [[2.25617293]\n",
      " [1.04501651]] b = [-26.48996976]\n",
      "step = 66800 error value = 0.07603347591174998 W = [[2.25909864]\n",
      " [1.04707012]] b = [-26.52859847]\n",
      "step = 67200 error value = 0.07566116426124657 W = [[2.26201028]\n",
      " [1.04911337]] b = [-26.56703864]\n",
      "step = 67600 error value = 0.07529245963326715 W = [[2.26490798]\n",
      " [1.05114638]] b = [-26.60529209]\n",
      "step = 68000 error value = 0.0749273100899781 W = [[2.26779188]\n",
      " [1.05316923]] b = [-26.64336061]\n",
      "step = 68400 error value = 0.07456566468257082 W = [[2.27066211]\n",
      " [1.05518203]] b = [-26.68124597]\n",
      "step = 68800 error value = 0.07420747342789706 W = [[2.27351879]\n",
      " [1.05718488]] b = [-26.71894991]\n",
      "step = 69200 error value = 0.07385268728574369 W = [[2.27636205]\n",
      " [1.05917788]] b = [-26.75647415]\n",
      "step = 69600 error value = 0.07350125813678422 W = [[2.27919203]\n",
      " [1.06116112]] b = [-26.79382038]\n",
      "step = 70000 error value = 0.07315313876111104 W = [[2.28200883]\n",
      " [1.0631347 ]] b = [-26.83099028]\n",
      "step = 70400 error value = 0.0728082828173658 W = [[2.28481258]\n",
      " [1.0650987 ]] b = [-26.86798549]\n",
      "step = 70800 error value = 0.07246664482246157 W = [[2.2876034 ]\n",
      " [1.06705323]] b = [-26.90480764]\n",
      "step = 71200 error value = 0.0721281801318709 W = [[2.2903814 ]\n",
      " [1.06899836]] b = [-26.94145831]\n",
      "step = 71600 error value = 0.07179284492041771 W = [[2.29314672]\n",
      " [1.0709342 ]] b = [-26.97793911]\n",
      "step = 72000 error value = 0.07146059616362817 W = [[2.29589945]\n",
      " [1.07286083]] b = [-27.01425158]\n",
      "step = 72400 error value = 0.0711313916195748 W = [[2.29863971]\n",
      " [1.07477833]] b = [-27.05039725]\n",
      "step = 72800 error value = 0.07080518981120021 W = [[2.30136762]\n",
      " [1.07668679]] b = [-27.08637766]\n",
      "step = 73200 error value = 0.07048195000913834 W = [[2.30408328]\n",
      " [1.07858629]] b = [-27.12219428]\n",
      "step = 73600 error value = 0.07016163221498985 W = [[2.3067868 ]\n",
      " [1.08047693]] b = [-27.1578486]\n",
      "step = 74000 error value = 0.06984419714502642 W = [[2.30947829]\n",
      " [1.08235877]] b = [-27.19334207]\n",
      "step = 74400 error value = 0.06952960621434562 W = [[2.31215786]\n",
      " [1.08423191]] b = [-27.22867613]\n",
      "step = 74800 error value = 0.06921782152144049 W = [[2.31482561]\n",
      " [1.08609641]] b = [-27.2638522]\n",
      "step = 75200 error value = 0.06890880583317309 W = [[2.31748164]\n",
      " [1.08795237]] b = [-27.29887166]\n",
      "step = 75600 error value = 0.06860252257014357 W = [[2.32012606]\n",
      " [1.08979985]] b = [-27.33373591]\n",
      "step = 76000 error value = 0.06829893579243916 W = [[2.32275896]\n",
      " [1.09163893]] b = [-27.3684463]\n",
      "step = 76400 error value = 0.06799801018576512 W = [[2.32538044]\n",
      " [1.09346969]] b = [-27.40300418]\n",
      "step = 76800 error value = 0.06769971104791575 W = [[2.32799062]\n",
      " [1.09529221]] b = [-27.43741088]\n",
      "step = 77200 error value = 0.06740400427559153 W = [[2.33058956]\n",
      " [1.09710655]] b = [-27.4716677]\n",
      "step = 77600 error value = 0.06711085635158712 W = [[2.33317739]\n",
      " [1.09891279]] b = [-27.50577594]\n",
      "step = 78000 error value = 0.06682023433227653 W = [[2.33575418]\n",
      " [1.10071101]] b = [-27.53973686]\n",
      "step = 78400 error value = 0.06653210583542488 W = [[2.33832004]\n",
      " [1.10250126]] b = [-27.57355174]\n",
      "step = 78800 error value = 0.0662464390283123 W = [[2.34087505]\n",
      " [1.10428363]] b = [-27.60722182]\n",
      "step = 79200 error value = 0.06596320261614742 W = [[2.3434193 ]\n",
      " [1.10605817]] b = [-27.64074832]\n",
      "step = 79600 error value = 0.06568236583079293 W = [[2.34595289]\n",
      " [1.10782496]] b = [-27.67413245]\n",
      "step = 80000 error value = 0.06540389841975158 W = [[2.34847591]\n",
      " [1.10958407]] b = [-27.70737542]\n"
     ]
    }
   ],
   "source": [
    "# 학습률\n",
    "learning_rate = 1e-2\n",
    "\n",
    "f = lambda x : loss_func(x_data, t_data)\n",
    "    \n",
    "for step in range(80001):\n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 400 == 0):\n",
    "        print(\"step =\", step, \"error value =\", error_val(x_data, t_data), \"W =\", W, \"b =\", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f8765788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14197058]), 0)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (예습, 복습) = 예습 3시간, 복습 17시간 일 때, Fail\n",
    "## 합격할 확률은 14% \n",
    "test_data = np.array([3, 17])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3ad5691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0008341]), 0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (예습, 복습) = 예습 5시간, 복습 8시간 일 때, Fail\n",
    "## 합격할 확률은 0.083%\n",
    "test_data = np.array([5, 8])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f00c5d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.99999406]), 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (예습, 복습) = 예습 7시간, 복습 21시간 일 때, Pass\n",
    "## 합격할 확률은 99.9%\n",
    "test_data = np.array([7, 21])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "368acb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.61640938]), 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (예습, 복습) = 예습 12시간, 복습 0시간 일 때, Pass\n",
    "## 합격할 확률은 61.6%\n",
    "test_data = np.array([12, 0])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836b23a",
   "metadata": {},
   "source": [
    "### 결과 해석\n",
    "- 복습보다는 예습시간이 합격(Pass)에 미치는 영향이 크다는 것을 알 수 있음\n",
    "- 예습시간 가중치 W1 = 2.3, 복습시간 가중치 W2 = 1.1 이므로 예습시간이 복습시간보다 약 2배이상 영향을 미치는 것을 알 수 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
